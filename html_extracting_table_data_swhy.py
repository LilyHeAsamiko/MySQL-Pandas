# -*- coding: utf-8 -*-
"""html_extracting_table_data_swhy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_UFzDzpHhiwifX6VmZ1-fri2A-NjVt5e

# Extracting table data from the web
"""

# Setup
import pandas as pd
import requests
import json
from bs4 import BeautifulSoup
import numpy as np
import time

"""## Get a web page and convert it to a dataframe"""

URL = "https://www.swhysc.com/swhyscyjzhPC/#/fundProduct "
#res = requests.get(URL, verify=False)
res = ''
while res == '':
  try:
    res = requests.get(URL, verify=False)
    break
  #  sys.exit()
  except:
    print('connection refuse by the server..')
    time.sleep(5)
    continue

print(res.status_code)

!pip install lxml
!pip install html5lib

#soup = BeautifulSoup(res.content, 'html.parser')
#soup = BeautifulSoup(res.text, 'html5lib')
soup = BeautifulSoup(res.content, 'lxml')


#tables = soup.find_all('table')
tables = soup.find('table', attr = {'class': 'tableSorter'})
print(np.shape(tables))
print(tables)
#table = tables[1] # note that the first table (at index 0) is not relevant

# Print the first 250 characters of the html table
#print(str(table)[:250])

data_list = pd.read_html(res.text)
df = data_list[0]
# Print the first 250 characters of the html table
print(str(table)[:250])

!pip install urllib

from urllib.request import urlopen, Request

import re
hdr = {'User-Agent':'Mozilla/5.0'}
req = Request(URL, headers = hdr)

Res = urlopen(req)
rawpage = Res.read().decode('utf-8')
soup = BeautifulSoup(rawpage, 'lxml')
print(soup)
#page = rawpage.replace('<!-->','')
#tb = page.find(lambda tag: tag.name == 'table' and tag.find(ttag.name == 'th'))
#tb = rawpage.find_all('table')

for tag in soup.find_all():
  if tag.name == "div":
    if str(tag).find("tableData___29y0D wrapauto___1JfTQ") > -1:
      print (repr(str(tag[:100])))
    else:
      print (str(tag).find("tableData___29y0D wrapauto___1JfTQ"))

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# # Ubuntu no longer distributes chromium-browser outside of snap
# #
# # Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap
# 
# # Add debian buster
# cat > /etc/apt/sources.list.d/debian.list <<'EOF'
# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main
# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main
# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main
# EOF
# 
# # Add keys
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A
# 
# apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg
# apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg
# apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg
# 
# # Prefer debian repo for chromium* packages only
# # Note the double-blank lines between entries
# cat > /etc/apt/preferences.d/chromium.pref << 'EOF'
# Package: *
# Pin: release a=eoan
# Pin-Priority: 500
# 
# 
# Package: *
# Pin: origin "deb.debian.org"
# Pin-Priority: 300
# 
# 
# Package: chromium*
# Pin: origin "deb.debian.org"
# Pin-Priority: 700
# EOF
# 
# # Install chromium and chromium-driver
# apt-get update
# apt-get install chromium chromium-driver
# 
# # Install selenium
# pip install selenium

!apt-get update 
!apt install chromium-chromedriver

!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

from selenium import webdriver
#options = webdriver.ChromeOptions()
#options.add_argument('--headless')
#options.add_argument('--no-sandbox')
#options.add_argument('--disable-dev-shm-usage')
#wd = webdriver.Chrome('chromedriver',options=options)
def web_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--verbose")
    options.add_argument('--no-sandbox')
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument("--window-size=1920, 1200")
    options.add_argument('--disable-dev-shm-usage')
    driver = webdriver.Chrome(options=options)
    return driver
#browser = webdriver.Chrome()
wd = web_driver()
wd.get(URL)
table = wd.find_element_by_css_selector('table')
for elem in table.find_element_by_css_selector('tr'):
  print('elem.text')



from selenium import webdriver
from selenium.webdriver.chrome.options import Options

options = Options()
options.add_argument("--headless")
options.add_argument("--no-sandbox")

options.headless = True

driver = webdriver.Chrome("/usr/bin/chromedriver", options=options)

driver.get(URL)
print(driver.title)

driver.quit()

!pip install selenium
!apt-get update # to update ubuntu to correctly run apt install
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver/usr/bin
import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')
from selenium import webdriver
options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
wd = webdriver.Chrome('chromedriver',options=options)

import pandas as pd
import csv

for i in range(1,185):  
	url = 'https://www.swhysc.com/swhyscproduct/service/wsfinmall/v1/fund/list?searchKey=&companyCode=&prodRiskRating=&productType=&fundNetValue=&saleStatus=&isFof=&sortFlag=0&pageNum='+(str(i))+'&pageSize=10'  
	tb = pd.read_html(url)
	tb.to_csv(r'C:\Users\Desktop\swhy.csv', mode='a', encoding='utf_8_sig', header=1, index=0)
	print('第'+str(i)+'页抓取完成')

import pandas as pd
import csv

def get_one_page(i):
  try:
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'
    }
    paras = {
		    'pageNum': i   #页码
		}
    url = 'https://www.swhysc.com/swhyscproduct/service/wsfinmall/v1/fund/list?searchKey=&companyCode=&prodRiskRating=&productType=&fundNetValue=&saleStatus=&isFof=&sortFlag=0&pageNum='+(str(i))+'&pageSize=10'  
    response = requests.get(url,headers = headers)
    if response.status_code == 200:
        return response.text
    return None
  except:
    print('爬取失败')

def parse_one_page(html):
    soup = BeautifulSoup(html,'lxml')
    content = soup.select('table')[0] 
    tbl = pd.read_html(content.prettify(),header = 0)[0]
    tbl.rename(columns = {'产品代码':'stock_code', '产品简称':'stock_abbre', '上市日期':'listing_date'， '净利润':'net_profit'， inplace = True)
    print(tbl)
	# return tbl
	# tbl = pd.DataFrame(tbl,dtype = 'object') 
    tbl.to_csv(r'C:\Users\Desktop\swhy2.csv', mode='a',encoding='utf_8_sig', header=1, index=0)
    
# 主函数
def main(page):
	for i in range(1,page):   # page表示提取页数
		html = get_one_page(i)
		parse_one_page(html)

# 单进程
if __name__ == '__main__':	
	main(50)   #共提取n页

"""## # Convert html to pandas dataframe"""

df = pd.read_html(str(table), header=0, flavor='html5lib')[0]
print(df.head(10).to_string())

"""## Convert dataframe to json string"""

str_json = (df.head(10).to_json(orient='records'))
print(str_json)

"""## Convert json string to list of dictionaries using 'json.loads'"""

li_json = json.loads(str_json)
for i,v in enumerate(li_json):
    print(i,v)